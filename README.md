# Algorithms-and-Big-O-notation C#
Big O Notation is a way to classify algorithms based on how their running time or memory requirements change as the size of the input to that algorithm changes.
As a programmers, we're always trying to design algorithms with the lowest complexity. We want our programs and our algorithms to grow gracefully whether it be 10 or 100 million elements that we're processing.
If we can't find the Big O notation for the algorithm we have chosen, we can get a ballpark estimate by looking quickly at the data structure and algorithm we have chosen.
